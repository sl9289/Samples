{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a36d546",
   "metadata": {
    "id": "fdb61851-4af6-4e85-b120-e7bfb03bb44e"
   },
   "source": [
    "# Web Scraping from Altmetric"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e17b9f",
   "metadata": {
    "id": "f4dfbaae-0d9b-4412-a953-7d6cdb827672"
   },
   "source": [
    "This code aims to scrape the latest information for all News, Blogs, Tweets and summary page from Altmetric, and detect the language for all headlines and subtitles. Then, this code also extracted detailed content for top 10 mediasource of News. The specific steps include:\n",
    "\n",
    "1) News and Blogs: Get all required information of news and blogs, detect their language and save them in separate data sets \"Altmetric_Blogs.xlsx\" and \"Altmetric_News.xlsx\". \n",
    "\n",
    "2) Twitter: Get the account handles (under a column called medialink) and headlines of all tweets, and delete our Twitter posts. Since tweets are always without any subtitles, I didn't add this column here. The final exported dataset is \"Altmetric_Tweets.xlsx\"\n",
    "\n",
    "3) Fetch the Altmetric scores, citations, readers and demographic information, and merged them with three existing data sets. After concatenating all date frames (), this code exports the final dataset as \"Altmetric_scrapedall.xlsx\".\n",
    "\n",
    "4) Scraped five of the top 10 news media sources for detailed content.\n",
    "\n",
    "There is no raw data set. And after the above processing, this code will export 4 new google sheets \"Altmetric_Blogs.xlsx\", \"Altmetric_News.xlsx\" (\"Altmetric_Tweets.xlsx\", and \"Altmetric_scrapedall.xlsx\", which will be also located in the \"Altmetric\" folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a4c085",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10892,
     "status": "ok",
     "timestamp": 1698184320844,
     "user": {
      "displayName": "Wanting Zhou",
      "userId": "17203249708928126809"
     },
     "user_tz": 420
    },
    "id": "c70Ih8A_IHv2",
    "outputId": "1ea772bb-e50c-4d10-d613-0403855d82fe",
    "tags": []
   },
   "outputs": [],
   "source": [
    "pip install langdetect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9271b91b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10682,
     "status": "ok",
     "timestamp": 1698184331524,
     "user": {
      "displayName": "Wanting Zhou",
      "userId": "17203249708928126809"
     },
     "user_tz": 420
    },
    "id": "trDqLPknIVUD",
    "outputId": "9ede6761-8b7d-4895-a500-29f33bec8043",
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "pip install pycountry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f3d733",
   "metadata": {
    "executionInfo": {
     "elapsed": 1016,
     "status": "ok",
     "timestamp": 1698184332520,
     "user": {
      "displayName": "Wanting Zhou",
      "userId": "17203249708928126809"
     },
     "user_tz": 420
    },
    "id": "OdrebCCiIdTP",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import needed packages and connect to google drive\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "from langdetect import detect\n",
    "from langdetect import DetectorFactory\n",
    "import pycountry\n",
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive')\n",
    "#%cd /content/drive/My Drive/Research/_Fiverr/Altmetric"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76845446",
   "metadata": {
    "id": "vWVrQVwPhsAy",
    "tags": []
   },
   "source": [
    "## Blogs\n",
    "### Step 1: Extract blog links, headlines and subtitles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91bbe721-30c4-4da8-b761-b117b4b9edc0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "urls = [\n",
    "    'https://oxfordjournals.altmetric.com/details/72683542',\n",
    "    'https://science.altmetric.com/details/60552876',\n",
    "    'https://jamanetwork.altmetric.com/details/64368646',\n",
    "    'https://science.altmetric.com/details/69584866',\n",
    "    'https://annals.altmetric.com/details/56459321',\n",
    "    'https://scienceadvances.altmetric.com/details/69530897',\n",
    "    'https://nature.altmetric.com/details/63584063'\n",
    "]\n",
    "\n",
    "author_map = {\n",
    "    'https://oxfordjournals.altmetric.com/details/72683542': 'Gangwisch et al.',\n",
    "    'https://science.altmetric.com/details/60552876': 'Lee et al.',\n",
    "    'https://jamanetwork.altmetric.com/details/64368646': 'Kim et al.',\n",
    "    'https://science.altmetric.com/details/69584866': 'Mina et al.',\n",
    "    'https://annals.altmetric.com/details/56459321': 'Hviid et al.',\n",
    "    'https://scienceadvances.altmetric.com/details/69530897': 'Maxwell et al.',\n",
    "    'https://nature.altmetric.com/details/63584063': 'Berzaghi et al.'\n",
    "}\n",
    "\n",
    "article_map = {\n",
    "    'https://oxfordjournals.altmetric.com/details/72683542': 'Article 2',\n",
    "    'https://science.altmetric.com/details/60552876': 'Article 3',\n",
    "    'https://jamanetwork.altmetric.com/details/64368646': 'Article 4',\n",
    "    'https://science.altmetric.com/details/69584866': 'Article 5',\n",
    "    'https://annals.altmetric.com/details/56459321': 'Article 6',\n",
    "    'https://scienceadvances.altmetric.com/details/69530897': 'Article 7',\n",
    "    'https://nature.altmetric.com/details/63584063': 'Article 8'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "646c317a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 21342,
     "status": "ok",
     "timestamp": 1698184353859,
     "user": {
      "displayName": "Wanting Zhou",
      "userId": "17203249708928126809"
     },
     "user_tz": 420
    },
    "id": "x-Yv3rL9MJu6",
    "outputId": "bd8d6c4b-244d-451b-d37e-b9c69c656a90",
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def fetch_blogs_and_mediasource(url):\n",
    "    blogs_and_mediasource = []\n",
    "    response = requests.get(url)\n",
    "\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to get URL {url}\")\n",
    "        return None\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    articles = soup.find_all('article', class_ = 'post blogs')\n",
    "\n",
    "    for article in articles:\n",
    "        blog_info = {}\n",
    "        h3_tag = article.find('h3')\n",
    "        h4_tag = article.find('h4')\n",
    "        p_tag = article.find('p', class_ = 'summary')\n",
    "        article_link = article.find('a', class_ = 'block_link')\n",
    "        time_tag = article.find('time', datetime = True)\n",
    "\n",
    "        if h3_tag:\n",
    "            blog_info['title'] = h3_tag.text.strip()\n",
    "\n",
    "        if h4_tag:\n",
    "            full_text = h4_tag.text.strip()\n",
    "            mediasource = full_text.split(\",\")[0]\n",
    "            blog_info['mediasource'] = mediasource\n",
    "\n",
    "        if p_tag:\n",
    "            blog_info['subtitle'] = p_tag.text.strip()\n",
    "\n",
    "        if time_tag:\n",
    "            blog_info['date'] = time_tag.text.strip()\n",
    "            \n",
    "        if article_link:\n",
    "            blog_info['url'] = article_link.get('href', 'N/A')\n",
    "        else:\n",
    "            blog_info['url'] = ''\n",
    "\n",
    "        if blog_info:\n",
    "            blogs_and_mediasource.append(blog_info)\n",
    "\n",
    "    return blogs_and_mediasource\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    blog_data = []\n",
    "    \n",
    "    for url in urls:\n",
    "        print(f\"Fetching data for URL: {url}\")\n",
    "        url1 = url + '/blogs'\n",
    "        result = fetch_blogs_and_mediasource(url1)\n",
    "\n",
    "        if result:\n",
    "            for blog in result:\n",
    "                row = {\n",
    "                    'altmetric': url,\n",
    "                    'article': article_map.get(url, 'N/A'),\n",
    "                    'author': author_map.get(url, 'N/A'),\n",
    "                    'mediatype': 'Blogs',\n",
    "                    'medialink': blog.get('url', 'N/A'),\n",
    "                    'mediasource': blog.get('mediasource', 'N/A'),\n",
    "                    'mediaheadline': blog.get('title', 'N/A'),\n",
    "                    'mediasubtitle': blog.get('subtitle', 'N/A'),\n",
    "                    'date': blog.get('date', 'N/A'),\n",
    "                }\n",
    "                blog_data.append(row)\n",
    "            print(\"Data fetched.\")\n",
    "        print(\"----------------------------\")\n",
    "\n",
    "    df_blog = pd.DataFrame(blog_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ff8f5f-f098-44e2-af04-a85d11f4871b",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Step 2: Detect language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "479a0cf8-72df-43cb-9f17-2c33827fc07b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def detect_language(text):\n",
    "    try:\n",
    "        return detect(text)\n",
    "    except:\n",
    "        return 'unknown'\n",
    "df_blog['language'] = df_blog['mediaheadline'].apply(detect_language)\n",
    "\n",
    "\n",
    "def get_language_name(lang_code):\n",
    "    try:\n",
    "        return pycountry.languages.get(alpha_2=lang_code).name\n",
    "    except AttributeError:\n",
    "        return \"Unknown\"\n",
    "\n",
    "df_blog['language'] = df_blog['language'].apply(get_language_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd6a11d-a86c-466b-84a2-e62607294740",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_blog"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60dd029a-1871-44be-84f9-c0bb739cb241",
   "metadata": {},
   "source": [
    "### Step 3: Export blogs dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef7b318-5f7e-437c-a576-4a2a8bd44c35",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#df_blog.to_excel('Altmetric_blogs.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5df29dd-8650-4ce7-8b2e-d793b0217d64",
   "metadata": {},
   "source": [
    "## News\n",
    "### Step 1: Find all news articles with an active link, headlines and subtitles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ff666f-10cf-4eaa-abb5-f86f1afc6d20",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def fetch_news_and_mediasource(base_url, page_number=1):\n",
    "    news_and_mediasource = []\n",
    "    url = f\"{base_url}/page:{page_number}\"\n",
    "    response = requests.get(url)\n",
    "\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to get URL {url}\")\n",
    "        return None, None\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    articles = soup.find_all('article', class_ = 'post msm')\n",
    "\n",
    "    for article in articles:\n",
    "        news_info = {}\n",
    "        h3_tag = article.find('h3')\n",
    "        h4_tag = article.find('h4')\n",
    "        p_tag = article.find('p', class_ = 'summary')\n",
    "        article_link = article.find('a', class_ = 'block_link')\n",
    "        time_tag = article.find('time', datetime = True)\n",
    "\n",
    "        if h3_tag:\n",
    "            news_info['title'] = h3_tag.text.strip()\n",
    "\n",
    "        if h4_tag:\n",
    "            full_text = h4_tag.text.strip()\n",
    "            mediasource = full_text.split(\",\")[0]\n",
    "            news_info['mediasource'] = mediasource\n",
    "\n",
    "        if p_tag:\n",
    "            news_info['subtitle'] = p_tag.text.strip()\n",
    "\n",
    "        if article_link:\n",
    "            news_info['url'] = article_link.get('href', 'N/A')\n",
    "        else:\n",
    "            news_info['url'] = ''\n",
    "        \n",
    "        if time_tag:\n",
    "            news_info['date'] = time_tag.text.strip()\n",
    "\n",
    "        if news_info:\n",
    "            news_and_mediasource.append(news_info)\n",
    "\n",
    "    next_page_tag = soup.find('a', class_ = 'next_page')\n",
    "    next_page_number = None\n",
    "\n",
    "    if next_page_tag:\n",
    "        next_page_url = next_page_tag.get('href')\n",
    "        next_page_number = next_page_url.split(\":\")[-1]\n",
    "\n",
    "    return news_and_mediasource, next_page_number\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    news_data = []\n",
    "\n",
    "    for base_url in urls:\n",
    "        base_url1 = base_url + '/news'\n",
    "        current_page = 1\n",
    "        while current_page:\n",
    "            print(f\"Fetching data for page: {current_page} from URL: {base_url}\")\n",
    "            result, next_page = fetch_news_and_mediasource(base_url1, current_page)\n",
    "\n",
    "            if result:\n",
    "                for news in result:\n",
    "                    row = {\n",
    "                        'altmetric': base_url,\n",
    "                        'article': article_map.get(base_url, 'N/A'),\n",
    "                        'author': author_map.get(base_url, 'N/A'),\n",
    "                        'mediatype': 'News',\n",
    "                        'medialink': news.get('url', 'N/A'),\n",
    "                        'mediasource': news.get('mediasource', 'N/A'),\n",
    "                        'mediaheadline': news.get('title', 'N/A'),\n",
    "                        'mediasubtitle': news.get('subtitle', 'N/A'),\n",
    "                        'date': news.get('date', 'N/A'),\n",
    "                    }\n",
    "                    news_data.append(row)\n",
    "                print(f\"Scraped {len(result)} news articles from page {current_page}\")\n",
    "\n",
    "            print(\"----------------------------\")\n",
    "            current_page = next_page\n",
    "            time.sleep(1)\n",
    "\n",
    "    df_news = pd.DataFrame(news_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef9a517b-1fe1-4081-aacc-f1ce6f6537e1",
   "metadata": {},
   "source": [
    "### Step 2: Detect language "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a3d7612-fc94-442a-b993-55ce1d6235e1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def detect_language(text):\n",
    "    try:\n",
    "        return detect(text)\n",
    "    except:\n",
    "        return 'unknown'\n",
    "df_news ['language'] = df_news ['mediaheadline'].apply(detect_language)\n",
    "\n",
    "\n",
    "def get_language_name(lang_code):\n",
    "    try:\n",
    "        return pycountry.languages.get(alpha_2=lang_code).name\n",
    "    except AttributeError:\n",
    "        return \"Unknown\"\n",
    "\n",
    "df_news ['language'] = df_news['language'].apply(get_language_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "719696ef-68c5-4d1a-8b0e-a180052180a0",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_news"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9194975-df4a-4e57-872b-9c04d5578be4",
   "metadata": {},
   "source": [
    "### Step 3: Export news dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae08f0e-8c73-4f37-9492-4659e87885c7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#df_news.to_excel('Altmetric_News.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33752553-a6c9-4633-8fbe-98f36805b9ad",
   "metadata": {},
   "source": [
    "## Tweets\n",
    "### Step 1: Extract all tweets and delete our posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e493aa3-9363-4714-845f-c5aa2e1059c7",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def fetch_tweets(base_url, page_number=1):\n",
    "    tweets = []\n",
    "    url = f\"{base_url}/page:{page_number}\"\n",
    "    response = requests.get(url)\n",
    "\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to get URL {url}\")\n",
    "        return None, None\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    articles = soup.find_all('article', class_ = 'post twitter')\n",
    "\n",
    "    for article in articles:\n",
    "        tweets_info = {}\n",
    "        handle_tag = author_handle = article.find('div', class_ = 'handle')\n",
    "        p_tag = article.find('p', class_ = 'summary')\n",
    "        time_tag = article.find('time', datetime = True)\n",
    "\n",
    "        if handle_tag:\n",
    "            tweets_info['title'] = handle_tag.text.strip()\n",
    "\n",
    "        if p_tag:\n",
    "            tweets_info['headline'] = p_tag.text.strip()\n",
    "            \n",
    "        if time_tag:\n",
    "            tweets_info['date'] = time_tag.text.strip()\n",
    "\n",
    "        if tweets_info:\n",
    "            tweets.append(tweets_info)\n",
    "\n",
    "    next_page_tag = soup.find('a', class_ = 'next_page')\n",
    "    next_page_number = None\n",
    "\n",
    "    if next_page_tag:\n",
    "        next_page_url = next_page_tag.get('href')\n",
    "        next_page_number = next_page_url.split(\":\")[-1]\n",
    "\n",
    "    return tweets, next_page_number\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    tweets_data = []\n",
    "\n",
    "    for base_url in urls:\n",
    "        base_url1 = base_url + '/twitter'\n",
    "        current_page = 1\n",
    "        while current_page:\n",
    "            print(f\"Fetching data for page: {current_page} from URL: {base_url}\")\n",
    "            result, next_page = fetch_tweets(base_url1, current_page)\n",
    "\n",
    "            if result:\n",
    "                for tweets in result:\n",
    "                    row = {\n",
    "                        'altmetric': base_url,\n",
    "                        'article': article_map.get(base_url, 'N/A'),\n",
    "                        'author': author_map.get(base_url, 'N/A'),\n",
    "                        'mediatype': 'Tweet',\n",
    "                        'medialink': tweets.get('title', 'N/A'),\n",
    "                        'mediasource': 'Twitter',\n",
    "                        'mediaheadline': tweets.get('headline', 'N/A'),\n",
    "                        'mediasubtitle': '',\n",
    "                        'date': tweets.get('date', 'N/A')\n",
    "                    }\n",
    "                    tweets_data.append(row)\n",
    "                print(f\"Scraped {len(result)} tweets from page {current_page}\")\n",
    "\n",
    "            print(\"----------------------------\")\n",
    "            current_page = next_page\n",
    "            time.sleep(1)\n",
    "\n",
    "    df_tweets = pd.DataFrame(tweets_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b953378-e98c-4509-8c4c-4aab6b9ced9b",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "0G1y0q0ZfmqI",
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Detele out twitter posts\n",
    "condition = df_tweets['medialink'].str.lower().str.contains('find|research')\n",
    "df_tweets_c = df_tweets[~condition]\n",
    "df_tweets_c = df_tweets_c.reset_index(drop = True)\n",
    "df_tweets_c"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e19007a1-0772-45be-b79d-e352154f69d5",
   "metadata": {},
   "source": [
    "### Step 2: Detect language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ce9c49",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "4d6edff7-147c-4832-b472-64406481a30b",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def detect_language(text):\n",
    "    try:\n",
    "        return detect(text)\n",
    "    except:\n",
    "        return 'unknown'\n",
    "df_tweets_c['language'] = df_tweets_c['mediaheadline'].apply(detect_language)\n",
    "\n",
    "\n",
    "def get_language_name(lang_code):\n",
    "    try:\n",
    "        return pycountry.languages.get(alpha_2=lang_code).name\n",
    "    except AttributeError:\n",
    "        return \"Unknown\"\n",
    "\n",
    "df_tweets_c['language'] = df_tweets_c['language'].apply(get_language_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f3a24f-cd3c-4b73-8a59-d79fcf04ca7f",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Step 3: Export tweets dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b519812-3bb9-44d8-b28e-9d8db9f9f60b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#df_tweets_c.to_excel('Altmetric_Tweets.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed35a01f-9dbc-457e-907a-a517be6873dc",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Demographic Information\n",
    "### Step 1: Scrape all demographic information for each article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3805624e-2a18-4b68-aec1-d3fc6533245c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "links = [\n",
    "    'https://oxfordjournals.altmetric.com/details/72683542', # Article 2 - Gangwisch\n",
    "    'https://science.altmetric.com/details/60552876', # Article 3 - Lee\n",
    "    'https://jamanetwork.altmetric.com/details/64368646', # Article 4 - Kim\n",
    "    'https://science.altmetric.com/details/69584866', # Article 5 - Mina\n",
    "    'https://annals.altmetric.com/details/56459321', # Article 6 - Hviid\n",
    "    'https://scienceadvances.altmetric.com/details/69530897', # Article 7 - Maxwell\n",
    "    'https://nature.altmetric.com/details/63584063', # Article 8 - Brezaghi\n",
    "]\n",
    "scores = []\n",
    "citations = []\n",
    "readers = []\n",
    "nums = []\n",
    "demo_public = []\n",
    "demo_scien = []\n",
    "demo_pract = []\n",
    "demo_sciencom = []\n",
    "demo_unknown = []\n",
    "\n",
    "def get_demo(type_name):\n",
    "    demo_list = []\n",
    "    found = False\n",
    "    for row in rows:\n",
    "        cell = row.find('td')\n",
    "        if cell and cell.get_text(strip = True) == type_name:\n",
    "            nums = [i.text for i in row.find_all('td', class_ = 'num')]\n",
    "            demo_list.extend(nums)\n",
    "            found = True\n",
    "    if not found:\n",
    "        demo_list.extend(['0', '0'])\n",
    "\n",
    "    return demo_list\n",
    "\n",
    "for index, link in enumerate (links, start = 2):\n",
    "    text = requests.get(link).text\n",
    "    soup = BeautifulSoup(text, 'html.parser')\n",
    "\n",
    "    # Get the Altmetric score\n",
    "    score = soup.find('div', class_ = 'altmetric-badge')\n",
    "    s = score['style']\n",
    "    s_start = s[s.find('score=')+6:]\n",
    "    s_end = s_start[:s_start.find('&')]\n",
    "    scores.append(s_end)\n",
    "\n",
    "\n",
    "    # Get citations\n",
    "    citation = soup.find('div', class_ = 'scholarly-citation-counts-wrapper')\n",
    "    c = citation.find('strong').text\n",
    "    citations.append(c)\n",
    "\n",
    "    # Get readers\n",
    "    reader = soup.find('div', class_ = 'reader-counts-wrapper')\n",
    "    r = reader.find('strong').text\n",
    "    readers.append(r)\n",
    "\n",
    "    # Get the demographics infomation\n",
    "    table = soup.find('div', class_ = 'table-wrapper users')\n",
    "    rows = table.select('.table-wrapper.users table tr')\n",
    "    \n",
    "    demo_public += get_demo(\"Members of the public\")\n",
    "    demo_scien += get_demo(\"Scientists\")\n",
    "    demo_pract += get_demo(\"Practitioners (doctors, other healthcare professionals)\")\n",
    "    demo_sciencom += get_demo(\"Science communicators (journalists, bloggers, editors)\")\n",
    "    demo_unknown += get_demo(\"Unknown\")\n",
    "\n",
    "df_s = pd.DataFrame({\n",
    "    'altmetric': links,\n",
    "    'demo_public_count': [demo_public[i] for i in range(0, len(demo_public), 2)],\n",
    "    'demo_public_perc': [demo_public[i] for i in range(1, len(demo_public), 2)],\n",
    "    'demo_scientist_count': [demo_scien[i] for i in range(0, len(demo_scien), 2)],\n",
    "    'demo_scientist_perc': [demo_scien[i] for i in range(1, len(demo_scien), 2)],\n",
    "    'demo_scientistCommunicator_count': [demo_sciencom[i] for i in range(0, len(demo_sciencom), 2)],\n",
    "    'demo_scientistCommunicator_perc': [demo_sciencom[i] for i in range(1, len(demo_sciencom), 2)],\n",
    "    'demo_practitioners_count': [demo_pract[i] for i in range(0, len(demo_pract), 2)],\n",
    "    'demo_practitioners_perc': [demo_pract[i] for i in range(1, len(demo_pract), 2)],\n",
    "    'demo_unkown_count': [demo_unknown[i] for i in range(0, len(demo_unknown), 2)],\n",
    "    'demo_unkown_perc': [demo_unknown[i] for i in range(1, len(demo_unknown), 2)],\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a02dea-a76d-43aa-a212-a060a206a5c9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf5b6fa-44b8-4cc7-8799-3a57e808b9e4",
   "metadata": {},
   "source": [
    "### Step 2: Merge the demographic dataset with blogs, news and tweets datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbde4fa6-e9a3-422e-a814-1dcd76a5e622",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plus_demo(dataset):\n",
    "    df_s['altmetric'] = df_s['altmetric'].astype(str)\n",
    "    dataset['altmetric'] = dataset['altmetric'].astype(str)\n",
    "    plus_demo = pd.merge(dataset, df_s, on = 'altmetric', how = 'left')\n",
    "    return plus_demo\n",
    "\n",
    "df_blog_demo = plus_demo(df_blog)\n",
    "df_news_demo = plus_demo(df_news)\n",
    "df_tweets_demo = plus_demo(df_tweets_c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1170326-154c-49c6-86c1-55a2bef8a02d",
   "metadata": {},
   "source": [
    "### Step 3: Concatenate three new datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7708a4c-870e-46cf-b8c2-d1407c0c808d",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_all = pd.concat([df_blog_demo, df_news_demo, df_tweets_demo])\n",
    "df_all"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d90db0-66af-4209-a48f-4ace51d6ef98",
   "metadata": {},
   "source": [
    "### Step 4: Export the final dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6e10720-2e20-4e91-a7c5-6b6e81700e2b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# df_all.to_excel('Altmetric_scrapedall.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93ce83f8-8272-457c-8bfc-783851ad938b",
   "metadata": {},
   "source": [
    "## Scraped five of the top 10 news media sources for detailed content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5307cd14-b02b-4868-87bb-01fc2d592198",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Scrap contents for yahoo!news\n",
    "links = df_news.loc[df_news['mediasource'] == 'Yahoo! News']['medialink']\n",
    "\n",
    "for link in links:\n",
    "    fetch = requests.get(link)\n",
    "    if fetch.status_code != 200:\n",
    "      print('This link is missing')\n",
    "      print('\\n----------------------------------------------\\n')\n",
    "      continue\n",
    "    text = fetch.text\n",
    "    soup = BeautifulSoup(text, 'html.parser')\n",
    "    article = soup.find('div', class_ = 'caas-body')\n",
    "    content = article.find_all('p')\n",
    "    for paragraph in content:\n",
    "      print(paragraph.text)\n",
    "    print('\\n----------------------------------------------\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd835c0-5452-402b-ad13-3aa033c68761",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Scrap titles for New York Times\n",
    "links = df_news.loc[df_news['mediasource'] == 'New York Times']['medialink']\n",
    "\n",
    "head = {\n",
    "      'User-Agent': 'Mozilla/5.0 (Linux; Android 6.0; Nexus 5 Build/MRA58N) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/111.0.0.0 Mobile Safari/537.36',\n",
    "      'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9',\n",
    "      'Accept-Charset': 'ISO-8859-1,utf-8;q=0.7,*;q=0.3',\n",
    "      'Accept-Encoding': 'none',\n",
    "      'Accept-Language': 'en-US,en;q=0.8',\n",
    "      'Connection': 'keep-alive',\n",
    "    }\n",
    "\n",
    "def fetch_link(link, head):\n",
    "    req = Request(link, headers = head)\n",
    "    return urlopen(req)\n",
    "\n",
    "def fetch_content(link):\n",
    "    content = fetch_link(link, head).read()\n",
    "    content = str(content, encoding ='utf8')\n",
    "    return content\n",
    "\n",
    "for link in links:\n",
    "    if (not type(link) is float) or (not math.isnan(link)):\n",
    "      text = fetch_content(link)\n",
    "      soup = BeautifulSoup(text, 'html.parser')\n",
    "      article = soup.find('div', class_ = \"css-s99gbd StoryBodyCompanionColumn\")\n",
    "      if article:\n",
    "        content = article.find_all('p')\n",
    "        for paragraph in content:\n",
    "          print(paragraph.text)\n",
    "        print('\\n----------------------------------------------\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12f2a4e7-2b2b-48ad-897a-c31e4abe0418",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Scrap titles for The Conversation\n",
    "links = df_news.loc[df_news['mediasource'] == 'The Conversation']['medialink']\n",
    "\n",
    "from urllib.request import Request, urlopen\n",
    "head = {\n",
    "      'User-Agent': 'Mozilla/5.0 (Linux; Android 6.0; Nexus 5 Build/MRA58N) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/111.0.0.0 Mobile Safari/537.36',\n",
    "      'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9',\n",
    "      'Accept-Charset': 'ISO-8859-1,utf-8;q=0.7,*;q=0.3',\n",
    "      'Accept-Encoding': 'none',\n",
    "      'Accept-Language': 'en-US,en;q=0.8',\n",
    "      'Connection': 'keep-alive',\n",
    "    }\n",
    "\n",
    "def fetch_link(link, head):\n",
    "    req = Request(link, headers = head)\n",
    "    return urlopen(req)\n",
    "\n",
    "def fetch_content(link):\n",
    "    content = fetch_link(link, head).read()\n",
    "    content = str(content, encoding ='utf8')\n",
    "    return content\n",
    "\n",
    "for link in links:\n",
    "    if (not type(link) is float) or (not math.isnan(link)):\n",
    "      text = fetch_content(link)\n",
    "      soup = BeautifulSoup(text, 'html.parser')\n",
    "      article = soup.find('div', class_ = \"grid-ten large-grid-nine grid-last content-body content entry-content instapaper_body inline-promos\")\n",
    "      if article:\n",
    "        content = article.find_all('p')\n",
    "        for paragraph in content:\n",
    "          print(paragraph.text)\n",
    "        print('\\n----------------------------------------------\\n')\n",
    "      else:\n",
    "        article = soup.find('div', class_ = \"grid-ten large-grid-nine grid-last content-body content entry-content instapaper_body\")\n",
    "        content = article.find_all('p')\n",
    "        for paragraph in content:\n",
    "          print(paragraph.text)\n",
    "        print('\\n----------------------------------------------\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e24fcd57-e0db-4cb8-8042-e6607eedcf52",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# The below two mediasources have the same websites layouts, so I use the same function for them\n",
    "def contents(name):\n",
    "    links = df_news.loc[df_news['mediasource'] == name]['medialink']\n",
    "\n",
    "    for link in links:\n",
    "        fetch = requests.get(link)\n",
    "        if fetch.status_code != 200:\n",
    "          print('This link is missing')\n",
    "          print('\\n----------------------------------------------\\n')\n",
    "          continue\n",
    "        text = fetch.text\n",
    "        soup = BeautifulSoup(text, 'html.parser')\n",
    "        article = soup.find('div', class_ = 'content')\n",
    "        content = article.find_all('p')\n",
    "        for paragraph in content:\n",
    "          print(paragraph.text)\n",
    "        print('\\n----------------------------------------------\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5269144-8b3d-4f60-bbd9-04f56db9b3c7",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Scrap contents for The Medical News\n",
    "contents('The Medical News')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a0a2f36-7c0a-4f1e-bb51-4272ce8070b4",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Scrap contents forNewsbreak\n",
    "contents('Newsbreak')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd0d9bc3-0917-475f-903d-400a9c9e7155",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "",
   "version": ""
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
